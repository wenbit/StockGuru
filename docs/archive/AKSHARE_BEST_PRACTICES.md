# ğŸ¯ å€Ÿé‰´ AKShare çš„æœ€ä½³å®è·µ

## ğŸ“‹ AKShare æ ¸å¿ƒè®¾è®¡åˆ†æ

é€šè¿‡æ·±å…¥åˆ†æ AKShare æºç ï¼ˆ10k+ starsï¼‰ï¼Œå‘ç°äº† **6ä¸ªä¼˜ç§€çš„è®¾è®¡æ¨¡å¼**ï¼š

---

## ğŸ—ï¸ æ ¸å¿ƒè®¾è®¡æ¨¡å¼

### 1. åˆ†å±‚å¼‚å¸¸å¤„ç†ä½“ç³» â­â­â­â­â­

#### è®¾è®¡æ€æƒ³
```python
# exceptions.py
class AkshareException(Exception):
    """åŸºç¡€å¼‚å¸¸ç±»"""
    def __init__(self, message):
        self.message = message
        super().__init__(self.message)

class APIError(AkshareException):
    """API è¯·æ±‚å¤±è´¥"""
    def __init__(self, message, status_code=None):
        self.status_code = status_code
        super().__init__(f"API Error: {message} (Status code: {status_code})")

class DataParsingError(AkshareException):
    """æ•°æ®è§£æå¤±è´¥"""
    pass

class NetworkError(AkshareException):
    """ç½‘ç»œç›¸å…³é—®é¢˜"""
    pass

class RateLimitError(AkshareException):
    """API é¢‘ç‡é™åˆ¶"""
    pass

class InvalidParameterError(AkshareException):
    """æ— æ•ˆå‚æ•°"""
    pass
```

#### æ ¸å¿ƒä¼˜åŠ¿
- âœ… **åˆ†ç±»æ¸…æ™°**: ä¸åŒç±»å‹çš„é”™è¯¯æœ‰ä¸“é—¨çš„å¼‚å¸¸ç±»
- âœ… **æ˜“äºå¤„ç†**: å¯ä»¥é’ˆå¯¹æ€§æ•è·å’Œå¤„ç†
- âœ… **ä¿¡æ¯ä¸°å¯Œ**: åŒ…å«çŠ¶æ€ç ç­‰è¯¦ç»†ä¿¡æ¯
- âœ… **ç»§æ‰¿ä½“ç³»**: ç»Ÿä¸€çš„åŸºç±»ä¾¿äºç»Ÿä¸€å¤„ç†

---

### 2. æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ â­â­â­â­â­

#### è®¾è®¡æ€æƒ³
```python
def make_request_with_retry_json(
    url, params=None, headers=None, proxies=None, 
    max_retries=3, retry_delay=1
):
    """
    å‘é€ HTTP GET è¯·æ±‚ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶å’Œä»£ç†è®¾ç½®
    """
    if proxies is None:
        proxies = config.proxies  # å…¨å±€ä»£ç†é…ç½®
    
    for attempt in range(max_retries):
        try:
            response = requests.get(
                url, params=params, headers=headers, proxies=proxies
            )
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    if not data:
                        raise DataParsingError("Empty response data")
                    return data
                except ValueError:
                    raise DataParsingError("Failed to parse JSON response")
            
            elif response.status_code == 429:
                raise RateLimitError(f"Rate limit exceeded")
            
            else:
                raise APIError(f"API request failed. Status code: {response.status_code}")
        
        except (RequestException, RateLimitError, APIError, DataParsingError) as e:
            if attempt == max_retries - 1:
                # æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if isinstance(e, RateLimitError):
                    raise
                elif isinstance(e, (APIError, DataParsingError)):
                    raise
                else:
                    raise NetworkError(
                        f"Failed to connect after {max_retries} attempts: {str(e)}"
                    )
            
            # æŒ‡æ•°é€€é¿ï¼šæ¯æ¬¡é‡è¯•ç­‰å¾…æ—¶é—´ç¿»å€
            time.sleep(retry_delay)
            retry_delay *= 2  # 1s -> 2s -> 4s
    
    raise NetworkError(f"Failed to connect after {max_retries} attempts")
```

#### æ ¸å¿ƒä¼˜åŠ¿
- âœ… **æŒ‡æ•°é€€é¿**: é¿å…é¢‘ç¹é‡è¯•å¯¼è‡´å°IP
- âœ… **æ™ºèƒ½é‡è¯•**: åŒºåˆ†å¯é‡è¯•å’Œä¸å¯é‡è¯•çš„é”™è¯¯
- âœ… **ä»£ç†é›†æˆ**: è‡ªåŠ¨ä½¿ç”¨å…¨å±€ä»£ç†é…ç½®
- âœ… **æ•°æ®éªŒè¯**: æ£€æŸ¥è¿”å›æ•°æ®æ˜¯å¦ä¸ºç©º

---

### 3. ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä»£ç† â­â­â­â­â­

#### è®¾è®¡æ€æƒ³
```python
# context.py
class AkshareConfig:
    """å…¨å±€é…ç½®å•ä¾‹"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.proxies = None
        return cls._instance
    
    @classmethod
    def set_proxies(cls, proxies):
        cls().proxies = proxies
    
    @classmethod
    def get_proxies(cls):
        return cls().proxies


class ProxyContext:
    """ä»£ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    def __init__(self, proxies):
        self.proxies = proxies
        self.old_proxies = None
    
    def __enter__(self):
        self.old_proxies = config.get_proxies()
        config.set_proxies(self.proxies)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        config.set_proxies(self.old_proxies)
        return False  # ä¸å¤„ç†å¼‚å¸¸


# ä½¿ç”¨ç¤ºä¾‹
with ProxyContext({'http': 'http://proxy:8080'}):
    # åœ¨è¿™ä¸ªä»£ç å—ä¸­ä½¿ç”¨ä»£ç†
    data = fetch_data()
# ä»£ç å—ç»“æŸåè‡ªåŠ¨æ¢å¤åŸä»£ç†é…ç½®
```

#### æ ¸å¿ƒä¼˜åŠ¿
- âœ… **ä¸´æ—¶ä»£ç†**: æ”¯æŒä¸´æ—¶åˆ‡æ¢ä»£ç†
- âœ… **è‡ªåŠ¨æ¢å¤**: é€€å‡ºæ—¶è‡ªåŠ¨æ¢å¤åŸé…ç½®
- âœ… **ä¼˜é›…è®¾è®¡**: ä½¿ç”¨ with è¯­å¥ï¼Œä»£ç æ¸…æ™°
- âœ… **çº¿ç¨‹å®‰å…¨**: å•ä¾‹æ¨¡å¼ä¿è¯å…¨å±€å”¯ä¸€

---

### 4. æ™ºèƒ½åˆ†é¡µè·å– â­â­â­â­â­

#### è®¾è®¡æ€æƒ³
```python
def fetch_paginated_data(url: str, base_params: Dict, timeout: int = 15):
    """
    ä¸œæ–¹è´¢å¯Œ-åˆ†é¡µè·å–æ•°æ®å¹¶åˆå¹¶ç»“æœ
    """
    # 1. å¤åˆ¶å‚æ•°ä»¥é¿å…ä¿®æ”¹åŸå§‹å‚æ•°
    params = base_params.copy()
    
    # 2. è·å–ç¬¬ä¸€é¡µæ•°æ®ï¼Œç”¨äºç¡®å®šåˆ†é¡µä¿¡æ¯
    r = requests.get(url, params=params, timeout=timeout)
    data_json = r.json()
    
    # 3. è®¡ç®—åˆ†é¡µä¿¡æ¯
    per_page_num = len(data_json["data"]["diff"])
    total_page = math.ceil(data_json["data"]["total"] / per_page_num)
    
    # 4. å­˜å‚¨æ‰€æœ‰é¡µé¢æ•°æ®
    temp_list = []
    temp_list.append(pd.DataFrame(data_json["data"]["diff"]))
    
    # 5. è·å–è¿›åº¦æ¡ï¼ˆè‡ªåŠ¨é€‚é…ç¯å¢ƒï¼‰
    tqdm = get_tqdm()
    
    # 6. è·å–å‰©ä½™é¡µé¢æ•°æ®
    for page in tqdm(range(2, total_page + 1), leave=False):
        params.update({"pn": page})
        r = requests.get(url, params=params, timeout=timeout)
        data_json = r.json()
        inner_temp_df = pd.DataFrame(data_json["data"]["diff"])
        temp_list.append(inner_temp_df)
    
    # 7. åˆå¹¶æ‰€æœ‰æ•°æ®
    temp_df = pd.concat(temp_list, ignore_index=True)
    temp_df["f3"] = pd.to_numeric(temp_df["f3"], errors="coerce")
    temp_df.sort_values(by=["f3"], ascending=False, inplace=True, ignore_index=True)
    temp_df.reset_index(inplace=True)
    temp_df["index"] = temp_df["index"].astype(int) + 1
    
    return temp_df
```

#### æ ¸å¿ƒä¼˜åŠ¿
- âœ… **è‡ªåŠ¨åˆ†é¡µ**: è‡ªåŠ¨è®¡ç®—æ€»é¡µæ•°
- âœ… **è¿›åº¦æ˜¾ç¤º**: æ˜¾ç¤ºè·å–è¿›åº¦
- âœ… **æ•°æ®åˆå¹¶**: è‡ªåŠ¨åˆå¹¶æ‰€æœ‰é¡µæ•°æ®
- âœ… **å‚æ•°éš”ç¦»**: ä¸ä¿®æ”¹åŸå§‹å‚æ•°

---

### 5. ç¯å¢ƒè‡ªé€‚åº”è¿›åº¦æ¡ â­â­â­â­

#### è®¾è®¡æ€æƒ³
```python
def get_tqdm(enable: bool = True):
    """
    è¿”å›é€‚ç”¨äºå½“å‰ç¯å¢ƒçš„ tqdm å¯¹è±¡
    """
    if not enable:
        # å¦‚æœè¿›åº¦æ¡è¢«ç¦ç”¨ï¼Œè¿”å›ä¸€ä¸ªä¸æ˜¾ç¤ºè¿›åº¦æ¡çš„å¯¹è±¡
        return lambda iterable, *args, **kwargs: iterable
    
    try:
        # å°è¯•æ£€æŸ¥æ˜¯å¦åœ¨ jupyter notebook ç¯å¢ƒä¸­
        shell = get_ipython().__class__.__name__
        if shell == "ZMQInteractiveShell":
            from tqdm.notebook import tqdm  # Jupyter ä¸“ç”¨
        else:
            from tqdm import tqdm  # æ ‡å‡†ç‰ˆæœ¬
    except (NameError, ImportError):
        # å¦‚æœä¸åœ¨ Jupyter ç¯å¢ƒä¸­ï¼Œå°±ä½¿ç”¨æ ‡å‡† tqdm
        from tqdm import tqdm
    
    return tqdm
```

#### æ ¸å¿ƒä¼˜åŠ¿
- âœ… **ç¯å¢ƒæ£€æµ‹**: è‡ªåŠ¨æ£€æµ‹ Jupyter ç¯å¢ƒ
- âœ… **è‡ªé€‚åº”**: ä½¿ç”¨åˆé€‚çš„è¿›åº¦æ¡ç‰ˆæœ¬
- âœ… **å¯ç¦ç”¨**: æ”¯æŒç¦ç”¨è¿›åº¦æ¡
- âœ… **ä¼˜é›…é™çº§**: æ£€æµ‹å¤±è´¥æ—¶ä½¿ç”¨æ ‡å‡†ç‰ˆæœ¬

---

### 6. ç»Ÿä¸€æ•°æ®æ¸…æ´— â­â­â­â­

#### è®¾è®¡æ€æƒ³
```python
def stock_zh_a_spot_em() -> pd.DataFrame:
    """ä¸œæ–¹è´¢å¯Œç½‘-æ²ªæ·±äº¬ A è‚¡-å®æ—¶è¡Œæƒ…"""
    # 1. è·å–æ•°æ®
    temp_df = fetch_paginated_data(url, params)
    
    # 2. è®¾ç½®åˆ—åï¼ˆä¸­æ–‡åˆ—åï¼Œæ˜“äºç†è§£ï¼‰
    temp_df.columns = [
        "åºå·", "ä»£ç ", "åç§°", "æœ€æ–°ä»·", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢",
        "æˆäº¤é‡", "æˆäº¤é¢", "æŒ¯å¹…", "æœ€é«˜", "æœ€ä½", "ä»Šå¼€", "æ˜¨æ”¶",
        "é‡æ¯”", "æ¢æ‰‹ç‡", "å¸‚ç›ˆç‡-åŠ¨æ€", "å¸‚å‡€ç‡", "æ€»å¸‚å€¼", 
        "æµé€šå¸‚å€¼", "æ¶¨é€Ÿ", "5åˆ†é’Ÿæ¶¨è·Œ", "60æ—¥æ¶¨è·Œå¹…", "å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…"
    ]
    
    # 3. ç»Ÿä¸€æ•°æ®ç±»å‹è½¬æ¢ï¼ˆä½¿ç”¨ errors="coerce" å¤„ç†å¼‚å¸¸å€¼ï¼‰
    numeric_cols = [
        "æœ€æ–°ä»·", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢", "æˆäº¤é‡", "æˆäº¤é¢", "æŒ¯å¹…",
        "æœ€é«˜", "æœ€ä½", "ä»Šå¼€", "æ˜¨æ”¶", "é‡æ¯”", "æ¢æ‰‹ç‡",
        "å¸‚ç›ˆç‡-åŠ¨æ€", "å¸‚å‡€ç‡", "æ€»å¸‚å€¼", "æµé€šå¸‚å€¼", "æ¶¨é€Ÿ",
        "5åˆ†é’Ÿæ¶¨è·Œ", "60æ—¥æ¶¨è·Œå¹…", "å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…"
    ]
    for col in numeric_cols:
        temp_df[col] = pd.to_numeric(temp_df[col], errors="coerce")
    
    # 4. é€‰æ‹©éœ€è¦çš„åˆ—ï¼ˆæ˜ç¡®çš„åˆ—é¡ºåºï¼‰
    temp_df = temp_df[[
        "åºå·", "ä»£ç ", "åç§°", "æœ€æ–°ä»·", "æ¶¨è·Œå¹…", "æ¶¨è·Œé¢",
        "æˆäº¤é‡", "æˆäº¤é¢", "æŒ¯å¹…", "æœ€é«˜", "æœ€ä½", "ä»Šå¼€", "æ˜¨æ”¶",
        "é‡æ¯”", "æ¢æ‰‹ç‡", "å¸‚ç›ˆç‡-åŠ¨æ€", "å¸‚å‡€ç‡", "æ€»å¸‚å€¼",
        "æµé€šå¸‚å€¼", "æ¶¨é€Ÿ", "5åˆ†é’Ÿæ¶¨è·Œ", "60æ—¥æ¶¨è·Œå¹…", "å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…"
    ]]
    
    return temp_df
```

#### æ ¸å¿ƒä¼˜åŠ¿
- âœ… **ä¸­æ–‡åˆ—å**: æ˜“äºç†è§£å’Œä½¿ç”¨
- âœ… **ç±»å‹å®‰å…¨**: ç»Ÿä¸€è½¬æ¢æ•°æ®ç±»å‹
- âœ… **å¼‚å¸¸å¤„ç†**: errors="coerce" å¤„ç†å¼‚å¸¸å€¼
- âœ… **åˆ—é¡ºåº**: æ˜ç¡®çš„åˆ—é¡ºåº

---

## ğŸš€ åº”ç”¨åˆ° StockGuru

### å®ç°æ–¹æ¡ˆ

#### 1. åˆ†å±‚å¼‚å¸¸å¤„ç†

```python
# app/exceptions.py

class StockGuruException(Exception):
    """åŸºç¡€å¼‚å¸¸ç±»"""
    def __init__(self, message):
        self.message = message
        super().__init__(self.message)


class DataSourceError(StockGuruException):
    """æ•°æ®æºé”™è¯¯"""
    def __init__(self, message, source_name=None):
        self.source_name = source_name
        super().__init__(f"[{source_name}] {message}" if source_name else message)


class DataValidationError(StockGuruException):
    """æ•°æ®éªŒè¯é”™è¯¯"""
    pass


class RateLimitError(StockGuruException):
    """é¢‘ç‡é™åˆ¶é”™è¯¯"""
    def __init__(self, message, retry_after=None):
        self.retry_after = retry_after
        super().__init__(message)


class DatabaseError(StockGuruException):
    """æ•°æ®åº“é”™è¯¯"""
    pass
```

---

#### 2. æŒ‡æ•°é€€é¿è¯·æ±‚å°è£…

```python
# app/services/smart_request.py

import time
import requests
from typing import Optional, Dict, Any
import logging

from app.exceptions import (
    DataSourceError, RateLimitError, DataValidationError
)

logger = logging.getLogger(__name__)


class SmartRequest:
    """æ™ºèƒ½è¯·æ±‚å°è£…ï¼ˆå€Ÿé‰´ AKShareï¼‰"""
    
    @staticmethod
    def get_json(
        url: str,
        params: Optional[Dict] = None,
        headers: Optional[Dict] = None,
        proxies: Optional[Dict] = None,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        timeout: int = 15
    ) -> Any:
        """
        å‘é€ GET è¯·æ±‚å¹¶è¿”å› JSON
        
        Args:
            url: è¯·æ±‚URL
            params: æŸ¥è¯¢å‚æ•°
            headers: è¯·æ±‚å¤´
            proxies: ä»£ç†é…ç½®
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            timeout: è¶…æ—¶æ—¶é—´
        
        Returns:
            JSON æ•°æ®
        
        Raises:
            DataSourceError: æ•°æ®æºé”™è¯¯
            RateLimitError: é¢‘ç‡é™åˆ¶
            DataValidationError: æ•°æ®éªŒè¯å¤±è´¥
        """
        current_delay = retry_delay
        
        for attempt in range(max_retries):
            try:
                response = requests.get(
                    url,
                    params=params,
                    headers=headers,
                    proxies=proxies,
                    timeout=timeout
                )
                
                # æˆåŠŸ
                if response.status_code == 200:
                    try:
                        data = response.json()
                        if not data:
                            raise DataValidationError("Empty response data")
                        return data
                    except ValueError as e:
                        raise DataValidationError(f"Failed to parse JSON: {e}")
                
                # é¢‘ç‡é™åˆ¶
                elif response.status_code == 429:
                    retry_after = response.headers.get('Retry-After', 60)
                    raise RateLimitError(
                        f"Rate limit exceeded",
                        retry_after=int(retry_after)
                    )
                
                # å…¶ä»–é”™è¯¯
                else:
                    raise DataSourceError(
                        f"Request failed with status {response.status_code}"
                    )
            
            except requests.exceptions.Timeout:
                logger.warning(f"Request timeout (attempt {attempt + 1}/{max_retries})")
            
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"Connection error (attempt {attempt + 1}/{max_retries}): {e}")
            
            except (RateLimitError, DataValidationError, DataSourceError) as e:
                if attempt == max_retries - 1:
                    raise
                logger.warning(f"Error (attempt {attempt + 1}/{max_retries}): {e}")
            
            # æœ€åä¸€æ¬¡å°è¯•
            if attempt == max_retries - 1:
                raise DataSourceError(
                    f"Failed after {max_retries} attempts"
                )
            
            # æŒ‡æ•°é€€é¿
            logger.info(f"Retrying in {current_delay}s...")
            time.sleep(current_delay)
            current_delay *= 2  # æŒ‡æ•°å¢é•¿
        
        raise DataSourceError(f"Failed after {max_retries} attempts")
```

---

#### 3. ä»£ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
# app/services/proxy_context.py

from contextlib import contextmanager
from typing import Optional, Dict
import logging

logger = logging.getLogger(__name__)


class GlobalConfig:
    """å…¨å±€é…ç½®ï¼ˆå•ä¾‹ï¼‰"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.proxies = None
        return cls._instance
    
    @classmethod
    def set_proxies(cls, proxies: Optional[Dict]):
        cls().proxies = proxies
        logger.info(f"Global proxies set: {proxies}")
    
    @classmethod
    def get_proxies(cls) -> Optional[Dict]:
        return cls().proxies


@contextmanager
def use_proxy(proxies: Dict):
    """
    ä¸´æ—¶ä½¿ç”¨ä»£ç†çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    
    Usage:
        with use_proxy({'http': 'http://proxy:8080'}):
            data = fetch_data()  # ä½¿ç”¨ä»£ç†
        # è‡ªåŠ¨æ¢å¤åŸé…ç½®
    """
    config = GlobalConfig()
    old_proxies = config.get_proxies()
    
    try:
        config.set_proxies(proxies)
        yield
    finally:
        config.set_proxies(old_proxies)
```

---

#### 4. æ™ºèƒ½åˆ†é¡µè·å–

```python
# app/services/paginated_fetcher.py

import math
from typing import Dict, List, Callable, Optional
import pandas as pd
from tqdm import tqdm
import logging

from app.services.smart_request import SmartRequest

logger = logging.getLogger(__name__)


class PaginatedFetcher:
    """æ™ºèƒ½åˆ†é¡µæ•°æ®è·å–å™¨"""
    
    @staticmethod
    def fetch_all_pages(
        url: str,
        base_params: Dict,
        page_param: str = 'page',
        page_size_param: str = 'page_size',
        data_extractor: Optional[Callable] = None,
        show_progress: bool = True
    ) -> pd.DataFrame:
        """
        è‡ªåŠ¨åˆ†é¡µè·å–æ‰€æœ‰æ•°æ®
        
        Args:
            url: è¯·æ±‚URL
            base_params: åŸºç¡€å‚æ•°
            page_param: é¡µç å‚æ•°å
            page_size_param: æ¯é¡µå¤§å°å‚æ•°å
            data_extractor: æ•°æ®æå–å‡½æ•°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
        Returns:
            åˆå¹¶åçš„ DataFrame
        """
        # 1. å¤åˆ¶å‚æ•°
        params = base_params.copy()
        
        # 2. è·å–ç¬¬ä¸€é¡µ
        first_page = SmartRequest.get_json(url, params=params)
        
        # 3. æå–æ•°æ®
        if data_extractor:
            total = data_extractor(first_page, 'total')
            data_list = data_extractor(first_page, 'data')
        else:
            total = first_page.get('total', 0)
            data_list = first_page.get('data', [])
        
        # 4. è®¡ç®—åˆ†é¡µ
        page_size = params.get(page_size_param, len(data_list))
        total_pages = math.ceil(total / page_size) if page_size > 0 else 1
        
        logger.info(f"Total records: {total}, Pages: {total_pages}")
        
        # 5. å­˜å‚¨æ•°æ®
        all_data = [pd.DataFrame(data_list)]
        
        # 6. è·å–å‰©ä½™é¡µ
        page_range = range(2, total_pages + 1)
        if show_progress:
            page_range = tqdm(page_range, desc="Fetching pages")
        
        for page in page_range:
            params[page_param] = page
            page_data = SmartRequest.get_json(url, params=params)
            
            if data_extractor:
                data_list = data_extractor(page_data, 'data')
            else:
                data_list = page_data.get('data', [])
            
            if data_list:
                all_data.append(pd.DataFrame(data_list))
        
        # 7. åˆå¹¶æ•°æ®
        if all_data:
            result_df = pd.concat(all_data, ignore_index=True)
            logger.info(f"Fetched {len(result_df)} records")
            return result_df
        
        return pd.DataFrame()
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡

| æŒ‡æ ‡ | å½“å‰ | ä¼˜åŒ–å | æå‡ |
|------|------|--------|------|
| é”™è¯¯å¤„ç† | åŸºç¡€ | **åˆ†å±‚** | âœ… |
| é‡è¯•æœºåˆ¶ | å›ºå®šå»¶è¿Ÿ | **æŒ‡æ•°é€€é¿** | âœ… |
| ä»£ç†ç®¡ç† | æ—  | **ä¸Šä¸‹æ–‡ç®¡ç†** | âœ… |
| åˆ†é¡µè·å– | æ‰‹åŠ¨ | **è‡ªåŠ¨** | âœ… |
| è¿›åº¦æ˜¾ç¤º | æ—  | **è‡ªé€‚åº”** | âœ… |

### ä»£ç è´¨é‡æå‡

- âœ… **å¼‚å¸¸å¤„ç†**: æ›´ç²¾ç¡®çš„é”™è¯¯åˆ†ç±»
- âœ… **é‡è¯•ç­–ç•¥**: æ™ºèƒ½çš„æŒ‡æ•°é€€é¿
- âœ… **ä»£ç†æ”¯æŒ**: çµæ´»çš„ä»£ç†åˆ‡æ¢
- âœ… **ç”¨æˆ·ä½“éªŒ**: è¿›åº¦æ¡æ˜¾ç¤º
- âœ… **ä»£ç å¤ç”¨**: é€šç”¨çš„å·¥å…·å‡½æ•°

---

## ğŸ¯ å®æ–½å»ºè®®

### ç«‹å³å®æ–½
1. âœ… åˆ†å±‚å¼‚å¸¸å¤„ç†ä½“ç³»
2. âœ… æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
3. âœ… ä»£ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨

### ä¸­æœŸå®æ–½
4. â³ æ™ºèƒ½åˆ†é¡µè·å–
5. â³ ç¯å¢ƒè‡ªé€‚åº”è¿›åº¦æ¡

---

## ğŸ’¡ æ ¸å¿ƒä»·å€¼

### AKShare vs AData å¯¹æ¯”

| ç‰¹æ€§ | AKShare | AData |
|------|---------|-------|
| å¼‚å¸¸å¤„ç† | â­â­â­â­â­ | â­â­â­ |
| é‡è¯•æœºåˆ¶ | â­â­â­â­â­ (æŒ‡æ•°é€€é¿) | â­â­â­â­ |
| ä»£ç†ç®¡ç† | â­â­â­â­â­ (ä¸Šä¸‹æ–‡) | â­â­â­â­ (å…¨å±€) |
| åˆ†é¡µè·å– | â­â­â­â­â­ | â­â­â­ |
| è¿›åº¦æ˜¾ç¤º | â­â­â­â­â­ | â­â­ |
| æ•°æ®æ¸…æ´— | â­â­â­â­â­ | â­â­â­â­ |

### ç»¼åˆå€Ÿé‰´

**æœ€ä½³æ–¹æ¡ˆ**: ç»“åˆä¸¤è€…ä¼˜åŠ¿
- **AData**: å¤šæ•°æ®æºèåˆã€æ¨¡æ¿æ–¹æ³•æ¨¡å¼
- **AKShare**: å¼‚å¸¸å¤„ç†ã€é‡è¯•æœºåˆ¶ã€åˆ†é¡µè·å–

---

**åˆ†æå®Œæˆæ—¶é—´**: 2025-10-17 08:35  
**æ ¸å¿ƒå€Ÿé‰´**: 6ä¸ªè®¾è®¡æ¨¡å¼  
**æ¨èå®æ–½**: åˆ†å±‚å¼‚å¸¸ + æŒ‡æ•°é€€é¿ + ä»£ç†ä¸Šä¸‹æ–‡
