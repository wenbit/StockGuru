# 📈 Neon 数据同步优化方案

## 一、存量数据初始化优化

### 当前方案问题
1. **串行获取数据**：baostock 不支持多线程，必须串行
2. **网络延迟**：每次 API 调用都有网络开销
3. **批量大小固定**：batch_size=500 可能不是最优值

### 优化方案 1.1：按月分批 + 并发日期

**思路**：虽然单个日期的股票必须串行获取，但可以并发处理多个日期

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import date, timedelta

def sync_historical_data_optimized(days: int = 365):
    """
    优化的历史数据同步
    - 按月分批
    - 每月数据并发处理
    """
    # 获取交易日列表
    trading_days = get_trading_days(days)
    
    # 按月分组
    months = group_by_month(trading_days)
    
    for month, dates in months.items():
        print(f"同步 {month} 的数据...")
        
        # 并发处理该月的多个日期
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {
                executor.submit(sync_single_day, date): date 
                for date in dates
            }
            
            for future in as_completed(futures):
                date = futures[future]
                try:
                    result = future.result()
                    print(f"{date} 完成: {result['success']} 条")
                except Exception as e:
                    print(f"{date} 失败: {e}")
```

**预期效果**：
- 3个日期并发：耗时减少 60-70%
- 从 100小时 → **30-40小时**

---

### 优化方案 1.2：增量 + 断点续传

**思路**：支持中断后继续，避免重复工作

```python
import json
from pathlib import Path

class IncrementalSync:
    """增量同步器"""
    
    def __init__(self, checkpoint_file='sync_checkpoint.json'):
        self.checkpoint_file = Path(checkpoint_file)
        self.checkpoint = self.load_checkpoint()
    
    def load_checkpoint(self):
        """加载检查点"""
        if self.checkpoint_file.exists():
            return json.loads(self.checkpoint_file.read_text())
        return {'completed_dates': [], 'failed_dates': []}
    
    def save_checkpoint(self):
        """保存检查点"""
        self.checkpoint_file.write_text(json.dumps(self.checkpoint, indent=2))
    
    def sync_with_resume(self, trading_days):
        """支持断点续传的同步"""
        completed = set(self.checkpoint['completed_dates'])
        remaining = [d for d in trading_days if d not in completed]
        
        print(f"总共 {len(trading_days)} 天，已完成 {len(completed)} 天")
        print(f"剩余 {len(remaining)} 天")
        
        for date in remaining:
            try:
                result = sync_single_day(date)
                self.checkpoint['completed_dates'].append(date)
                self.save_checkpoint()
                print(f"✅ {date} 完成")
            except Exception as e:
                self.checkpoint['failed_dates'].append(date)
                self.save_checkpoint()
                print(f"❌ {date} 失败: {e}")
```

**预期效果**：
- 支持中断恢复
- 避免重复工作
- 提高可靠性

---

### 优化方案 1.3：智能批量大小

**思路**：根据网络状况动态调整 batch_size

```python
class AdaptiveBatchSync:
    """自适应批量同步"""
    
    def __init__(self):
        self.batch_size = 500
        self.min_batch = 100
        self.max_batch = 1000
        self.success_rate = 1.0
    
    def adjust_batch_size(self, success_count, total_count):
        """根据成功率调整批量大小"""
        self.success_rate = success_count / total_count if total_count > 0 else 1.0
        
        if self.success_rate > 0.95:
            # 成功率高，增加批量
            self.batch_size = min(self.batch_size + 100, self.max_batch)
        elif self.success_rate < 0.85:
            # 成功率低，减少批量
            self.batch_size = max(self.batch_size - 100, self.min_batch)
        
        return self.batch_size
    
    def sync_with_adaptive_batch(self, stocks, date_str):
        """自适应批量同步"""
        total_stocks = len(stocks)
        success_count = 0
        
        for i in range(0, total_stocks, self.batch_size):
            batch = stocks[i:i + self.batch_size]
            batch_success = sync_batch(batch, date_str)
            success_count += batch_success
            
            # 动态调整
            new_batch_size = self.adjust_batch_size(batch_success, len(batch))
            print(f"批量大小调整: {self.batch_size} → {new_batch_size}")
```

**预期效果**：
- 网络好时：更大批量，更快速度
- 网络差时：更小批量，更稳定
- 自动优化，无需手动调整

---

## 二、增量数据同步优化

### 当前方案问题
1. **固定时间执行**：每天22点，可能不是最优时间
2. **无重试机制**：失败后需要手动重试
3. **无监控告警**：同步失败不知道

### 优化方案 2.1：智能调度

**思路**：根据数据可用性智能选择执行时间

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

class SmartScheduler:
    """智能调度器"""
    
    def __init__(self):
        self.scheduler = AsyncIOScheduler()
    
    def setup_smart_schedule(self):
        """设置智能调度"""
        
        # 方案1: 多次尝试
        # 晚上8点首次尝试
        self.scheduler.add_job(
            self.try_sync,
            CronTrigger(hour=20, minute=0),
            id='sync_attempt_1'
        )
        
        # 晚上10点二次尝试
        self.scheduler.add_job(
            self.try_sync,
            CronTrigger(hour=22, minute=0),
            id='sync_attempt_2'
        )
        
        # 早上8点最后尝试
        self.scheduler.add_job(
            self.try_sync,
            CronTrigger(hour=8, minute=0),
            id='sync_attempt_3'
        )
    
    async def try_sync(self):
        """尝试同步"""
        yesterday = date.today() - timedelta(days=1)
        
        # 检查是否已同步
        if await self.is_synced(yesterday):
            print(f"{yesterday} 已同步，跳过")
            return
        
        # 检查是否为交易日
        if not await self.is_trading_day(yesterday):
            print(f"{yesterday} 非交易日，跳过")
            return
        
        # 执行同步
        result = await self.sync_daily_data(yesterday)
        
        if result['status'] == 'success':
            print(f"✅ {yesterday} 同步成功")
            # 取消后续尝试
            self.cancel_remaining_attempts()
        else:
            print(f"⚠️ {yesterday} 同步失败，等待下次尝试")
```

**预期效果**：
- 提高成功率（多次尝试）
- 自动跳过已同步日期
- 减少人工干预

---

### 优化方案 2.2：失败重试机制

**思路**：自动重试失败的股票

```python
from tenacity import retry, stop_after_attempt, wait_exponential

class RetryableSync:
    """支持重试的同步"""
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def fetch_stock_with_retry(self, stock, date_str):
        """带重试的股票数据获取"""
        return self.fetch_stock_daily_data(stock, date_str)
    
    def sync_with_retry(self, stocks, date_str):
        """支持重试的同步"""
        failed_stocks = []
        
        # 第一轮：正常同步
        for stock in stocks:
            try:
                data = self.fetch_stock_with_retry(stock, date_str)
                self.insert_data(data)
            except Exception as e:
                failed_stocks.append(stock)
                print(f"❌ {stock['code']} 失败: {e}")
        
        # 第二轮：重试失败的
        if failed_stocks:
            print(f"\n重试 {len(failed_stocks)} 只失败的股票...")
            time.sleep(5)  # 等待5秒
            
            for stock in failed_stocks:
                try:
                    data = self.fetch_stock_with_retry(stock, date_str)
                    self.insert_data(data)
                    print(f"✅ {stock['code']} 重试成功")
                except Exception as e:
                    print(f"❌ {stock['code']} 重试仍失败: {e}")
```

**预期效果**：
- 提高成功率（从 95% → 99%+）
- 自动处理临时网络问题
- 减少数据缺失

---

### 优化方案 2.3：监控和告警

**思路**：实时监控同步状态，失败时发送通知

```python
import smtplib
from email.mime.text import MIMEText

class SyncMonitor:
    """同步监控器"""
    
    def __init__(self):
        self.metrics = {
            'total_syncs': 0,
            'success_syncs': 0,
            'failed_syncs': 0,
            'avg_duration': 0
        }
    
    def record_sync(self, result):
        """记录同步结果"""
        self.metrics['total_syncs'] += 1
        
        if result['status'] == 'success':
            self.metrics['success_syncs'] += 1
        else:
            self.metrics['failed_syncs'] += 1
            # 发送告警
            self.send_alert(result)
        
        # 更新平均耗时
        duration = result.get('duration', 0)
        self.metrics['avg_duration'] = (
            (self.metrics['avg_duration'] * (self.metrics['total_syncs'] - 1) + duration)
            / self.metrics['total_syncs']
        )
    
    def send_alert(self, result):
        """发送告警"""
        message = f"""
        ⚠️ 数据同步失败
        
        日期: {result['date']}
        错误: {result.get('error', 'Unknown')}
        成功: {result.get('success', 0)}
        失败: {result.get('failed', 0)}
        
        请检查日志: /var/log/stockguru/sync.log
        """
        
        # 发送邮件/企业微信/钉钉等
        self.send_notification(message)
    
    def get_health_status(self):
        """获取健康状态"""
        success_rate = (
            self.metrics['success_syncs'] / self.metrics['total_syncs']
            if self.metrics['total_syncs'] > 0 else 1.0
        )
        
        return {
            'status': 'healthy' if success_rate > 0.95 else 'warning',
            'success_rate': f"{success_rate * 100:.2f}%",
            'avg_duration': f"{self.metrics['avg_duration']:.2f}s",
            'total_syncs': self.metrics['total_syncs']
        }
```

**预期效果**：
- 及时发现问题
- 自动告警通知
- 数据可视化监控

---

## 三、通用优化

### 3.1 连接池优化

**当前问题**：每次同步都创建新连接

**优化方案**：使用连接池

```python
from psycopg2 import pool

class ConnectionPoolManager:
    """连接池管理器"""
    
    def __init__(self, database_url, min_conn=2, max_conn=10):
        self.pool = pool.ThreadedConnectionPool(
            min_conn,
            max_conn,
            database_url
        )
    
    def get_connection(self):
        """获取连接"""
        return self.pool.getconn()
    
    def return_connection(self, conn):
        """归还连接"""
        self.pool.putconn(conn)
    
    def close_all(self):
        """关闭所有连接"""
        self.pool.closeall()

# 使用示例
pool_manager = ConnectionPoolManager(DATABASE_URL)

def sync_with_pool(data):
    conn = pool_manager.get_connection()
    try:
        # 执行同步
        cursor = conn.cursor()
        execute_values(cursor, sql, data)
        conn.commit()
    finally:
        pool_manager.return_connection(conn)
```

**预期效果**：
- 减少连接开销
- 提高并发性能
- 更稳定的连接

---

### 3.2 数据压缩

**思路**：减少网络传输量

```python
import gzip
import pickle

def compress_data(data):
    """压缩数据"""
    pickled = pickle.dumps(data)
    compressed = gzip.compress(pickled)
    return compressed

def decompress_data(compressed):
    """解压数据"""
    pickled = gzip.decompress(compressed)
    data = pickle.loads(pickled)
    return data
```

**预期效果**：
- 减少 60-70% 传输量
- 加快网络传输
- 降低带宽成本

---

### 3.3 增量更新策略

**思路**：只更新变化的数据

```python
def incremental_update(new_data, date_str):
    """增量更新"""
    conn = get_connection()
    cursor = conn.cursor()
    
    # 1. 查询已存在的数据
    cursor.execute("""
        SELECT stock_code, close_price, volume
        FROM daily_stock_data
        WHERE trade_date = %s
    """, (date_str,))
    
    existing = {row[0]: row for row in cursor.fetchall()}
    
    # 2. 找出需要更新的数据
    to_insert = []
    to_update = []
    
    for record in new_data:
        code = record['stock_code']
        if code not in existing:
            to_insert.append(record)
        elif has_changed(record, existing[code]):
            to_update.append(record)
    
    # 3. 批量插入和更新
    if to_insert:
        execute_values(cursor, insert_sql, to_insert)
    
    if to_update:
        execute_values(cursor, update_sql, to_update)
    
    conn.commit()
    
    print(f"新增: {len(to_insert)}, 更新: {len(to_update)}")
```

**预期效果**：
- 减少不必要的写入
- 提高更新速度
- 降低数据库负载

---

## 四、推荐实施顺序

### 阶段 1：立即实施（高优先级）
1. ✅ **失败重试机制**（方案 2.2）
   - 实施难度：低
   - 效果：显著提高成功率

2. ✅ **监控告警**（方案 2.3）
   - 实施难度：中
   - 效果：及时发现问题

3. ✅ **连接池优化**（方案 3.1）
   - 实施难度：低
   - 效果：提升性能

### 阶段 2：短期实施（中优先级）
4. ⏳ **智能调度**（方案 2.1）
   - 实施难度：中
   - 效果：提高可靠性

5. ⏳ **断点续传**（方案 1.2）
   - 实施难度：中
   - 效果：支持中断恢复

### 阶段 3：长期优化（低优先级）
6. 📅 **按月并发**（方案 1.1）
   - 实施难度：高
   - 效果：大幅提升初始化速度

7. 📅 **自适应批量**（方案 1.3）
   - 实施难度：中
   - 效果：自动优化性能

---

## 五、预期效果总结

| 优化项 | 当前性能 | 优化后 | 提升 |
|--------|---------|--------|------|
| **存量初始化** | 100小时 | 30-40小时 | 2.5-3x |
| **增量同步** | 14分钟 | 10-12分钟 | 1.2-1.4x |
| **成功率** | 95% | 99%+ | +4% |
| **可靠性** | 中 | 高 | ⭐⭐⭐ |
| **可维护性** | 中 | 高 | ⭐⭐⭐ |

---

## 六、实施建议

### 最小可行方案（MVP）
1. 失败重试机制
2. 基础监控日志
3. 连接池优化

**预计工作量**: 2-3 天  
**预期收益**: 成功率 +4%, 性能 +20%

### 完整方案
实施所有优化

**预计工作量**: 1-2 周  
**预期收益**: 性能提升 2-3倍，可靠性大幅提升

---

**建议**: 先实施阶段1的高优先级优化，验证效果后再考虑后续优化。
