"""
ç»ˆæä¼˜åŒ–è·å–å™¨
ä½¿ç”¨é¢„å– + å¹¶è¡Œå¤„ç† + æ‰¹é‡æ’å…¥
é¢„æœŸæé€Ÿ 2å€
"""

import logging
import time
import threading
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor
import pandas as pd

logger = logging.getLogger(__name__)


class OptimizedFetcher:
    """
    ç»ˆæä¼˜åŒ–è·å–å™¨
    - é¢„å–ä¸‹ä¸€ä¸ªæ•°æ®ï¼ˆéšè—ç½‘ç»œå»¶è¿Ÿï¼‰
    - å¹¶è¡Œå¤„ç†æ•°æ®ï¼ˆCPUå¯†é›†å‹ï¼‰
    - æ‰¹é‡å‡†å¤‡æ’å…¥
    """
    
    def __init__(self):
        self.cache = {}
        self.cache_lock = threading.Lock()
        logger.info("âœ… Optimized fetcher initialized")
    
    def fetch_all_optimized(
        self,
        stock_codes: List[str],
        date_str: str,
        progress_callback=None
    ) -> List[Tuple[str, pd.DataFrame]]:
        """
        ä¼˜åŒ–çš„æ‰¹é‡è·å–
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            progress_callback: è¿›åº¦å›è°ƒ
        
        Returns:
            (code, DataFrame) å…ƒç»„åˆ—è¡¨
        """
        import baostock as bs
        
        logger.info(f"ğŸš€ Starting optimized fetch for {len(stock_codes)} stocks")
        
        # 1. åªç™»å½•ä¸€æ¬¡
        bs.login()
        start_time = time.time()
        
        raw_data = []
        success_count = 0
        
        try:
            # 2. ä¸²è¡Œè·å– + é¢„å–
            for i, code in enumerate(stock_codes):
                try:
                    # è·å–å½“å‰æ•°æ®ï¼ˆå¯èƒ½ä»ç¼“å­˜ï¼‰
                    df = self._fetch_with_cache(code, date_str, bs)
                    
                    if not df.empty:
                        raw_data.append((code, df))
                        success_count += 1
                    
                    # é¢„å–ä¸‹ä¸€ä¸ªï¼ˆå¦‚æœæœ‰ï¼‰
                    if i + 1 < len(stock_codes):
                        next_code = stock_codes[i + 1]
                        self._prefetch_async(next_code, date_str, bs)
                    
                    # è¿›åº¦å›è°ƒ
                    if progress_callback and ((i + 1) % 100 == 0 or i + 1 == len(stock_codes)):
                        progress_callback(i + 1, len(stock_codes), code)
                
                except Exception as e:
                    logger.warning(f"Failed to fetch {code}: {e}")
            
            fetch_time = time.time() - start_time
            
            logger.info(
                f"âœ… Fetch completed: {success_count}/{len(stock_codes)} "
                f"in {fetch_time:.2f}s ({len(stock_codes)/fetch_time:.1f} stocks/s)"
            )
        
        finally:
            # 3. åªç™»å‡ºä¸€æ¬¡
            bs.logout()
        
        return raw_data
    
    def _fetch_with_cache(self, code: str, date_str: str, bs) -> pd.DataFrame:
        """
        å¸¦ç¼“å­˜çš„è·å–
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            date_str: æ—¥æœŸ
            bs: baostock å®ä¾‹
        
        Returns:
            DataFrame
        """
        key = f"{code}_{date_str}"
        
        # æ£€æŸ¥ç¼“å­˜
        with self.cache_lock:
            if key in self.cache:
                logger.debug(f"Cache hit: {code}")
                return self.cache.pop(key)
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œç›´æ¥è·å–
        return self._fetch_single(code, date_str, bs)
    
    def _fetch_single(self, code: str, date_str: str, bs) -> pd.DataFrame:
        """
        è·å–å•åªè‚¡ç¥¨
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            date_str: æ—¥æœŸ
            bs: baostock å®ä¾‹
        
        Returns:
            DataFrame
        """
        try:
            prefix = "sh." if code.startswith('6') else "sz."
            rs = bs.query_history_k_data_plus(
                f"{prefix}{code}",
                "date,code,open,high,low,close,volume,amount,turn,pctChg",
                start_date=date_str,
                end_date=date_str
            )
            
            data = []
            while rs.error_code == '0' and rs.next():
                data.append(rs.get_row_data())
            
            if data:
                df = pd.DataFrame(data, columns=rs.fields)
                return df
            
            return pd.DataFrame()
        
        except Exception as e:
            logger.error(f"Error fetching {code}: {e}")
            return pd.DataFrame()
    
    def _prefetch_async(self, code: str, date_str: str, bs):
        """
        å¼‚æ­¥é¢„å–ä¸‹ä¸€ä¸ªæ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            date_str: æ—¥æœŸ
            bs: baostock å®ä¾‹
        """
        def prefetch():
            try:
                df = self._fetch_single(code, date_str, bs)
                
                if not df.empty:
                    key = f"{code}_{date_str}"
                    with self.cache_lock:
                        self.cache[key] = df
                        logger.debug(f"Prefetched: {code}")
            
            except Exception as e:
                logger.warning(f"Prefetch failed for {code}: {e}")
        
        # å¯åŠ¨é¢„å–çº¿ç¨‹
        thread = threading.Thread(target=prefetch, daemon=True)
        thread.start()
    
    def process_data_parallel(
        self,
        raw_data: List[Tuple[str, pd.DataFrame]],
        max_workers: int = 4
    ) -> List[tuple]:
        """
        å¹¶è¡Œå¤„ç†æ•°æ®
        
        Args:
            raw_data: åŸå§‹æ•°æ®åˆ—è¡¨
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        
        Returns:
            å‡†å¤‡å¥½çš„æ•°æ®åº“æ’å…¥å…ƒç»„åˆ—è¡¨
        """
        logger.info(f"ğŸ”„ Processing {len(raw_data)} records with {max_workers} workers")
        
        start_time = time.time()
        
        # å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(self._process_single, raw_data))
        
        # å±•å¹³ç»“æœ
        data_to_insert = []
        for result in results:
            if result:
                data_to_insert.extend(result)
        
        process_time = time.time() - start_time
        
        logger.info(
            f"âœ… Processing completed: {len(data_to_insert)} records "
            f"in {process_time:.2f}s"
        )
        
        return data_to_insert
    
    def _process_single(self, item: Tuple[str, pd.DataFrame]) -> List[tuple]:
        """
        å¤„ç†å•ä¸ªæ•°æ®
        
        Args:
            item: (code, DataFrame) å…ƒç»„
        
        Returns:
            æ•°æ®åº“æ’å…¥å…ƒç»„åˆ—è¡¨
        """
        code, df = item
        
        if df.empty:
            return []
        
        result = []
        
        try:
            for _, row in df.iterrows():
                result.append((
                    code,  # stock_code
                    '',  # stock_name
                    row.get('date', ''),  # trade_date
                    float(row.get('open', 0)),  # open_price
                    float(row.get('close', 0)),  # close_price
                    float(row.get('high', 0)),  # high_price
                    float(row.get('low', 0)),  # low_price
                    int(float(row.get('volume', 0))),  # volume
                    float(row.get('amount', 0)),  # amount
                    float(row.get('pctChg', 0)),  # change_pct
                    float(row.get('turn', 0))  # turnover_rate
                ))
        
        except Exception as e:
            logger.warning(f"Failed to process {code}: {e}")
        
        return result


# å…¨å±€å®ä¾‹
optimized_fetcher = OptimizedFetcher()
